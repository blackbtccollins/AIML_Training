{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fashion MNIST Competition",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/blackbtccollins/AIML_Training/blob/main/class/Blackburn_submission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uHwxGemYV_FT"
      },
      "source": [
        "# Fashion MNIST Competition! - Blackburn Submission\n",
        "**Author**: T. Blackburn\n",
        "\n",
        "**Updates**: New Version\n",
        "\n",
        "## Problem\n",
        "\n",
        "Classify images from the \"Fashion MNIST\" data set.   Optimize the test accuracy.\n",
        "\n",
        "## Metrics\n",
        "\n",
        "This competition is evaluated on the mean Dice coefficient. The Dice coefficient can be used to compare the pixel-wise agreement between a predicted segmentation and its corresponding ground truth. The formula is given by:![alt text](https://user-images.githubusercontent.com/26015273/41822460-2ca0a90a-77f0-11e8-9c71-7e88fa6b5c61.gif)\n",
        "\n",
        "\n",
        "The double sum is over the observations `i`, whose number is `N`, and the categories `c`, whose number is `C`. The term `1_{y_i \\in C_c}` is the indicator function of the `i`th observation belonging to the `c`th category. The `p_{model}[y_i \\in C_c]` is the probability predicted by the model for the `i`th observation to belong to the `c`th category. When there are more than two categories, the neural network outputs a vector of `C` probabilities, each giving the probability that the network input should be classified as belonging to the respective category. When the number of categories is just two, the neural network outputs a single probability `\\hat{y}_i`, with the other one being `1` minus the output. This is why the binary cross entropy looks a bit different from categorical cross entropy, despite being a special case of it.\n",
        "\n",
        "## Dataset\n",
        "\n",
        "This dataset is the Fashion MNIST dataset\n",
        "\n",
        "Recently, the researchers at Zalando, an e-commerce company, introduced Fashion MNIST as a drop-in replacement for the original MNIST dataset. Like MNIST, Fashion MNIST consists of a training set consisting of 60,000 examples belonging to 10 different classes and a test set of 10,000 examples. Each training example is a gray-scale image, 28x28 in size. The authors of the work further claim that the Fashion MNIST should actually replace MNIST dataset for benchmarking of new Machine Learning or Computer Vision models.\n",
        "\n",
        "Each image is 28 pixels in height and 28 pixels in width, for a total of 784 pixels in total. Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. This pixel-value is an integer between 0 and 255. The training and test data sets have 785 columns. The first column consists of the class labels (see above), and represents the article of clothing. The rest of the columns contain the pixel-values of the associated image.\n",
        "\n",
        "The Labels are:  \n",
        "0 T-shirt/top\n",
        "1 Trouser\n",
        "2 Pullover\n",
        "3 Dress\n",
        "4 Coat\n",
        "5 Sandal\n",
        "6 Shirt\n",
        "7 Sneaker\n",
        "8 Bag\n",
        "9 Ankle boot \n",
        "\n",
        "## Objective\n",
        "\n",
        "In this competition, you can try different variations of the CNN model given as a reference, you may evaluate techniques to squeeze more performance out of a CNN, or you might even try a completely different model, neural network or otherwise.  You will note that there are tips/tricks/techniques documented in many locations on the internet that could be useful.\n",
        "\n",
        "## Rules and Timeline\n",
        "\n",
        "The primary measure for the competition will be the accuracy of prediction on the test data.  Ties will be broken by Precision accuracy first, then Recall Accuracy if needed.\n",
        "\n",
        "The results will be revealed at the end of the last day of class.  Please submit your Metrics blocks (Starts with SUBMIT... and ends with END SUBMISSION) to instructors (wtnewman@raytheon.com) before lunch.\n",
        "\n",
        "A prize will be given to the top finisher(s) based on the judgement of the instructor and the availability of prizes.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0A2_Mo0KxWc"
      },
      "source": [
        "from tensorflow import keras\n",
        "from keras.datasets import fashion_mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten,  Conv2D, MaxPooling2D, Activation, BatchNormalization\n",
        "from keras import backend as K\n",
        "from keras.callbacks import EarlyStopping\n",
        "from sklearn import metrics\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.callbacks import TensorBoard,  ModelCheckpoint\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "import os\n",
        "%matplotlib inline\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = \"2\""
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3y6VjlzZLKPB"
      },
      "source": [
        "## Set Up Variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LtL1H333LHGz"
      },
      "source": [
        "epochs = 150                 # Number of Training Epochs\n",
        "num_classes = 10            # This is the number of classes in the Fashion MNIST dataset\n",
        "batch_size = 200          # This parameter can be adjusted\n",
        "img_rows, img_cols = 28, 28 # Pixel sizes of the Images in the Dataset"
      ],
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SW52B7dT9H5"
      },
      "source": [
        "## Gather and Process Fashion MNIST data\n",
        "\n",
        "1. First, collect the data from Keras (our goal is someday that our organizational data is this easy to get!)\n",
        "2. Then split into train and test sets.\n",
        "3. Next we need to process the data into the proper shape for the CNN\n",
        "4. Then scale the floats to land between 0 and 1.  Often times we use sklearn's MinMaxScaler for this, but in this case we're going for simplicity.\n",
        "5. Next take the y_train and y_test labels and encode them one-hot.  This will enable the CNN to function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GZENio2YLPUy",
        "outputId": "49f27448-fafd-4393-e423-24b30a4fa8fd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Grab the data from the keras repository\n",
        "\n",
        "mnist_data = fashion_mnist.load_data()\n",
        "x = mnist_data[0][0]\n",
        "y = mnist_data[0][1]\n",
        "\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.33, random_state=41)\n",
        "\n",
        "# Process the date into the right tensor shape.  This is a good practice, but\n",
        "# usually tensorflow uses channels last (the 'else' here)\n",
        "\n",
        "if K.image_data_format() == \"channels first\":\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)\n",
        "    \n",
        "#\n",
        "#  Cast to a 32 bit float and then scale so the value is a float between 0 and 1\n",
        "    \n",
        "x_train = x_train.astype(\"float32\")\n",
        "x_test = x_test.astype(\"float32\")\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "#\n",
        "# Convert Class Vector to Binary Class Matrices (one-hot encoding).\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
        "print(y_test.shape)\n",
        "\n",
        "#\n",
        "# Function to decode one-hot encoding later on when we want to evaluate performance.\n",
        "def decode_one_hot(y):\n",
        "    y_classes = [np.argmax(yi, axis=None, out=None) for yi in y]\n",
        "    return y_classes\n",
        "\n",
        "'''\n",
        "\n",
        "Below we're experimenting with the Keras ImageDataGenerator.  From my experience, if these parameters\n",
        "are set too aggressively, the loss/accuracy will either never improve or it will take too long to improve.\n",
        "Below is an example of a complex data augmentation regime.  This is just for reference.  See my more simple\n",
        "one at the bottom.\n",
        "\n",
        "    \n",
        "datagen = ImageDataGenerator(rotation_range=0.5, \n",
        "                                 zoom_range=0.1,\n",
        "                                 featurewise_center=True,\n",
        "                                 #featurewise_std_normalization=True,\n",
        "                                 width_shift_range=0.1, \n",
        "                                 height_shift_range=0.1, \n",
        "                                 shear_range=0.1,\n",
        "                                 horizontal_flip=True, \n",
        "                                 fill_mode=\"nearest\")\n",
        "'''\n",
        "#\n",
        "#  Set up our Image Augmentation Data Generator\n",
        "#\n",
        "datagen = ImageDataGenerator(rotation_range=5)\n",
        "datagen.fit(x_train)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19800, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FcFnUPDLvnO"
      },
      "source": [
        "## Build the Model\n",
        "\n",
        "* In this example, we define the below block as a Sequential Model. \n",
        "* See the excellent [Keras Documentation](https://keras.io/guides/sequential_model/) on Sequential Models for info.\n",
        "* Many of these parameters can be experimented with.  The documentation will help you understand how much to experiment."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8EWdo8mALXB9",
        "outputId": "8daf2cd5-cbc9-4b44-a0bc-3b58a64ed008",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "#\n",
        "# This is what is known as a Tensorflow (Keras) Sequential model\n",
        "# We will talk at some level about each of these layer types in class.\n",
        "#\n",
        "\n",
        "###Best####\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 #kernel_initializer='he_normal',\n",
        "                 input_shape=input_shape))\n",
        "model.add(Dropout(0.7))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Conv2D(64, kernel_size=(3,3)))\n",
        "model.add(LeakyReLU(alpha=0.05))\n",
        "model.add(Dropout(0.6))\n",
        "model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(800))  \n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "my_callbacks = [ModelCheckpoint('model_out.hdf5', monitor='val_acc',  mode='max', save_best_only=True, period=1)]\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# This is what is known as a Tensorflow (Keras) Sequential model\n",
        "# We will talk at some level about each of these layer types in class.\n",
        "#\n",
        "\n",
        "###BEST - Save ####\n",
        "\n",
        "#model = Sequential()\n",
        "#\n",
        "#model.add(Conv2D(32, kernel_size=(3,3), input_shape=input_shape))\n",
        "#model.add(Dropout(0.3))\n",
        "#model.add(Activation('relu'))\n",
        "#model.add(Conv2D(64, kernel_size=(3,3)))\n",
        "#model.add(LeakyReLU(alpha=0.05))\n",
        "#model.add(Dropout(0.2))\n",
        "#model.add(MaxPooling2D(pool_size=(2,2)))\n",
        "#model.add(Activation('relu'))\n",
        "#model.add(Flatten())\n",
        "#model.add(Dense(100))  \n",
        "#model.add(Activation('relu'))\n",
        "#model.add(Dropout(0.1))\n",
        "#model.add(Dense(num_classes, activation=\"softmax\"))\n",
        "#\n",
        "#my_callbacks = [ModelCheckpoint('model_out.hdf5', monitor='acc', save_best_only=True, period=1)]"
      ],
      "metadata": {
        "id": "BUyaq5L-Q1Di"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvIfjap-L8TO"
      },
      "source": [
        "## Fit and Evaluate the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5U20dV1L-eP",
        "outputId": "11f7abbc-6db7-4703-8907-3a6d1657ccb1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Flag to determine whether we use Keras' Image augmentation data generator\n",
        "augmentation = False\n",
        "\n",
        "#\n",
        "# Compile the model so we can fit it. Researching loss functions and optimizers\n",
        "# might be a good thing to do.\n",
        "#\n",
        "model.compile(loss=keras.losses.categorical_crossentropy, \n",
        "              optimizer=keras.optimizers.Adam(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if not augmentation:\n",
        "    #\n",
        "    # Fit the model.  Once the model is trained we'll evaluate the performance.\n",
        "    print('not using image augmentation')\n",
        "    hist = model.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=my_callbacks)\n",
        "else:\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "    hist = model.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                           steps_per_epoch=len(x_train) / batch_size, validation_data=(x_test, y_test),\n",
        "                           epochs=epochs, verbose=1, callbacks=my_callbacks, workers = 2)\n",
        "\n",
        "\n",
        "score = model.evaluate(x_test, y_test)\n",
        "\n",
        "#\n",
        "# Predict on the test data and pass to metrics function\n",
        "yhat = np.argmax(model.predict(x_test), axis=-1)\n",
        "y_dec = decode_one_hot(y_test)\n",
        "\n",
        "print(\"\\nSUBMIT THIS BLOCK for the Competition\\n\")\n",
        "print(metrics.classification_report(y_dec, yhat))\n",
        "print(\"Testing Loss:\", score[0])\n",
        "print(\"Testing Accuracy:\", score[1])\n",
        "print(\"END SUBMISSION BLOCK\\n\")\n",
        "\n",
        "model.summary()\n",
        "\n"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not using image augmentation\n",
            "Epoch 1/150\n",
            "201/201 [==============================] - 5s 24ms/step - loss: 0.6485 - accuracy: 0.7657 - val_loss: 0.5761 - val_accuracy: 0.8490\n",
            "Epoch 2/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.4128 - accuracy: 0.8490 - val_loss: 0.4626 - val_accuracy: 0.8727\n",
            "Epoch 3/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.3596 - accuracy: 0.8687 - val_loss: 0.3782 - val_accuracy: 0.8843\n",
            "Epoch 4/150\n",
            "201/201 [==============================] - 5s 22ms/step - loss: 0.3295 - accuracy: 0.8780 - val_loss: 0.3464 - val_accuracy: 0.8925\n",
            "Epoch 5/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.3040 - accuracy: 0.8866 - val_loss: 0.3287 - val_accuracy: 0.8976\n",
            "Epoch 6/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.2860 - accuracy: 0.8956 - val_loss: 0.3082 - val_accuracy: 0.8952\n",
            "Epoch 7/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.2702 - accuracy: 0.8988 - val_loss: 0.3016 - val_accuracy: 0.9049\n",
            "Epoch 8/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.2564 - accuracy: 0.9041 - val_loss: 0.2774 - val_accuracy: 0.9065\n",
            "Epoch 9/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.2428 - accuracy: 0.9101 - val_loss: 0.2757 - val_accuracy: 0.9092\n",
            "Epoch 10/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.2326 - accuracy: 0.9130 - val_loss: 0.2711 - val_accuracy: 0.9086\n",
            "Epoch 11/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.2239 - accuracy: 0.9163 - val_loss: 0.2592 - val_accuracy: 0.9140\n",
            "Epoch 12/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.2118 - accuracy: 0.9204 - val_loss: 0.2531 - val_accuracy: 0.9134\n",
            "Epoch 13/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.2056 - accuracy: 0.9230 - val_loss: 0.2407 - val_accuracy: 0.9171\n",
            "Epoch 14/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.2014 - accuracy: 0.9240 - val_loss: 0.2525 - val_accuracy: 0.9119\n",
            "Epoch 15/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1916 - accuracy: 0.9277 - val_loss: 0.2459 - val_accuracy: 0.9157\n",
            "Epoch 16/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1828 - accuracy: 0.9300 - val_loss: 0.2348 - val_accuracy: 0.9189\n",
            "Epoch 17/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1773 - accuracy: 0.9345 - val_loss: 0.2258 - val_accuracy: 0.9199\n",
            "Epoch 18/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1710 - accuracy: 0.9347 - val_loss: 0.2266 - val_accuracy: 0.9205\n",
            "Epoch 19/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1675 - accuracy: 0.9364 - val_loss: 0.2208 - val_accuracy: 0.9234\n",
            "Epoch 20/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1622 - accuracy: 0.9382 - val_loss: 0.2292 - val_accuracy: 0.9176\n",
            "Epoch 21/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1547 - accuracy: 0.9421 - val_loss: 0.2249 - val_accuracy: 0.9214\n",
            "Epoch 22/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1511 - accuracy: 0.9427 - val_loss: 0.2176 - val_accuracy: 0.9225\n",
            "Epoch 23/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1438 - accuracy: 0.9447 - val_loss: 0.2163 - val_accuracy: 0.9233\n",
            "Epoch 24/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1428 - accuracy: 0.9461 - val_loss: 0.2152 - val_accuracy: 0.9232\n",
            "Epoch 25/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1382 - accuracy: 0.9485 - val_loss: 0.2211 - val_accuracy: 0.9199\n",
            "Epoch 26/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1314 - accuracy: 0.9505 - val_loss: 0.2137 - val_accuracy: 0.9246\n",
            "Epoch 27/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.1295 - accuracy: 0.9526 - val_loss: 0.2198 - val_accuracy: 0.9217\n",
            "Epoch 28/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1263 - accuracy: 0.9526 - val_loss: 0.2213 - val_accuracy: 0.9220\n",
            "Epoch 29/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1245 - accuracy: 0.9531 - val_loss: 0.2123 - val_accuracy: 0.9251\n",
            "Epoch 30/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1181 - accuracy: 0.9551 - val_loss: 0.2155 - val_accuracy: 0.9234\n",
            "Epoch 31/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1191 - accuracy: 0.9531 - val_loss: 0.2149 - val_accuracy: 0.9252\n",
            "Epoch 32/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.1134 - accuracy: 0.9572 - val_loss: 0.2216 - val_accuracy: 0.9224\n",
            "Epoch 33/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.1124 - accuracy: 0.9578 - val_loss: 0.2132 - val_accuracy: 0.9260\n",
            "Epoch 34/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1078 - accuracy: 0.9589 - val_loss: 0.2160 - val_accuracy: 0.9245\n",
            "Epoch 35/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1080 - accuracy: 0.9587 - val_loss: 0.2189 - val_accuracy: 0.9231\n",
            "Epoch 36/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.1075 - accuracy: 0.9594 - val_loss: 0.2155 - val_accuracy: 0.9261\n",
            "Epoch 37/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1061 - accuracy: 0.9589 - val_loss: 0.2177 - val_accuracy: 0.9255\n",
            "Epoch 38/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.1013 - accuracy: 0.9613 - val_loss: 0.2217 - val_accuracy: 0.9253\n",
            "Epoch 39/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0988 - accuracy: 0.9632 - val_loss: 0.2144 - val_accuracy: 0.9256\n",
            "Epoch 40/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0962 - accuracy: 0.9639 - val_loss: 0.2162 - val_accuracy: 0.9265\n",
            "Epoch 41/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0928 - accuracy: 0.9640 - val_loss: 0.2218 - val_accuracy: 0.9259\n",
            "Epoch 42/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0932 - accuracy: 0.9648 - val_loss: 0.2285 - val_accuracy: 0.9243\n",
            "Epoch 43/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0881 - accuracy: 0.9665 - val_loss: 0.2166 - val_accuracy: 0.9286\n",
            "Epoch 44/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0937 - accuracy: 0.9641 - val_loss: 0.2152 - val_accuracy: 0.9274\n",
            "Epoch 45/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0890 - accuracy: 0.9669 - val_loss: 0.2183 - val_accuracy: 0.9276\n",
            "Epoch 46/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0861 - accuracy: 0.9683 - val_loss: 0.2268 - val_accuracy: 0.9265\n",
            "Epoch 47/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0896 - accuracy: 0.9658 - val_loss: 0.2260 - val_accuracy: 0.9263\n",
            "Epoch 48/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0877 - accuracy: 0.9678 - val_loss: 0.2212 - val_accuracy: 0.9277\n",
            "Epoch 49/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0841 - accuracy: 0.9684 - val_loss: 0.2279 - val_accuracy: 0.9251\n",
            "Epoch 50/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0826 - accuracy: 0.9682 - val_loss: 0.2152 - val_accuracy: 0.9289\n",
            "Epoch 51/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0821 - accuracy: 0.9699 - val_loss: 0.2211 - val_accuracy: 0.9276\n",
            "Epoch 52/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.0821 - accuracy: 0.9692 - val_loss: 0.2192 - val_accuracy: 0.9290\n",
            "Epoch 53/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.0795 - accuracy: 0.9709 - val_loss: 0.2229 - val_accuracy: 0.9295\n",
            "Epoch 54/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0738 - accuracy: 0.9732 - val_loss: 0.2286 - val_accuracy: 0.9261\n",
            "Epoch 55/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0798 - accuracy: 0.9698 - val_loss: 0.2336 - val_accuracy: 0.9280\n",
            "Epoch 56/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0767 - accuracy: 0.9713 - val_loss: 0.2321 - val_accuracy: 0.9277\n",
            "Epoch 57/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0762 - accuracy: 0.9721 - val_loss: 0.2333 - val_accuracy: 0.9243\n",
            "Epoch 58/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0751 - accuracy: 0.9724 - val_loss: 0.2299 - val_accuracy: 0.9259\n",
            "Epoch 59/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0713 - accuracy: 0.9735 - val_loss: 0.2254 - val_accuracy: 0.9290\n",
            "Epoch 60/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0734 - accuracy: 0.9723 - val_loss: 0.2472 - val_accuracy: 0.9236\n",
            "Epoch 61/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0734 - accuracy: 0.9725 - val_loss: 0.2399 - val_accuracy: 0.9263\n",
            "Epoch 62/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0760 - accuracy: 0.9726 - val_loss: 0.2395 - val_accuracy: 0.9264\n",
            "Epoch 63/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0711 - accuracy: 0.9738 - val_loss: 0.2295 - val_accuracy: 0.9284\n",
            "Epoch 64/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0688 - accuracy: 0.9738 - val_loss: 0.2339 - val_accuracy: 0.9283\n",
            "Epoch 65/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0656 - accuracy: 0.9771 - val_loss: 0.2433 - val_accuracy: 0.9257\n",
            "Epoch 66/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0700 - accuracy: 0.9741 - val_loss: 0.2457 - val_accuracy: 0.9271\n",
            "Epoch 67/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0734 - accuracy: 0.9731 - val_loss: 0.2297 - val_accuracy: 0.9290\n",
            "Epoch 68/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0667 - accuracy: 0.9756 - val_loss: 0.2337 - val_accuracy: 0.9284\n",
            "Epoch 69/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0646 - accuracy: 0.9763 - val_loss: 0.2343 - val_accuracy: 0.9276\n",
            "Epoch 70/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0656 - accuracy: 0.9761 - val_loss: 0.2343 - val_accuracy: 0.9294\n",
            "Epoch 71/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0673 - accuracy: 0.9759 - val_loss: 0.2345 - val_accuracy: 0.9286\n",
            "Epoch 72/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0660 - accuracy: 0.9756 - val_loss: 0.2389 - val_accuracy: 0.9257\n",
            "Epoch 73/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0670 - accuracy: 0.9756 - val_loss: 0.2375 - val_accuracy: 0.9281\n",
            "Epoch 74/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0650 - accuracy: 0.9762 - val_loss: 0.2390 - val_accuracy: 0.9274\n",
            "Epoch 75/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0681 - accuracy: 0.9755 - val_loss: 0.2436 - val_accuracy: 0.9265\n",
            "Epoch 76/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0614 - accuracy: 0.9774 - val_loss: 0.2484 - val_accuracy: 0.9269\n",
            "Epoch 77/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0640 - accuracy: 0.9766 - val_loss: 0.2475 - val_accuracy: 0.9264\n",
            "Epoch 78/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0622 - accuracy: 0.9777 - val_loss: 0.2428 - val_accuracy: 0.9272\n",
            "Epoch 79/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0623 - accuracy: 0.9761 - val_loss: 0.2437 - val_accuracy: 0.9288\n",
            "Epoch 80/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0591 - accuracy: 0.9792 - val_loss: 0.2599 - val_accuracy: 0.9274\n",
            "Epoch 81/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0631 - accuracy: 0.9773 - val_loss: 0.2442 - val_accuracy: 0.9272\n",
            "Epoch 82/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0604 - accuracy: 0.9783 - val_loss: 0.2425 - val_accuracy: 0.9278\n",
            "Epoch 83/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0618 - accuracy: 0.9775 - val_loss: 0.2377 - val_accuracy: 0.9292\n",
            "Epoch 84/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0593 - accuracy: 0.9786 - val_loss: 0.2532 - val_accuracy: 0.9271\n",
            "Epoch 85/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0605 - accuracy: 0.9788 - val_loss: 0.2364 - val_accuracy: 0.9283\n",
            "Epoch 86/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0592 - accuracy: 0.9792 - val_loss: 0.2441 - val_accuracy: 0.9271\n",
            "Epoch 87/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0571 - accuracy: 0.9791 - val_loss: 0.2473 - val_accuracy: 0.9287\n",
            "Epoch 88/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0597 - accuracy: 0.9783 - val_loss: 0.2475 - val_accuracy: 0.9276\n",
            "Epoch 89/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0600 - accuracy: 0.9784 - val_loss: 0.2475 - val_accuracy: 0.9281\n",
            "Epoch 90/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0590 - accuracy: 0.9780 - val_loss: 0.2551 - val_accuracy: 0.9264\n",
            "Epoch 91/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0578 - accuracy: 0.9802 - val_loss: 0.2553 - val_accuracy: 0.9292\n",
            "Epoch 92/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0536 - accuracy: 0.9805 - val_loss: 0.2450 - val_accuracy: 0.9282\n",
            "Epoch 93/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0593 - accuracy: 0.9789 - val_loss: 0.2508 - val_accuracy: 0.9279\n",
            "Epoch 94/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0566 - accuracy: 0.9793 - val_loss: 0.2498 - val_accuracy: 0.9302\n",
            "Epoch 95/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0578 - accuracy: 0.9781 - val_loss: 0.2544 - val_accuracy: 0.9276\n",
            "Epoch 96/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0539 - accuracy: 0.9808 - val_loss: 0.2499 - val_accuracy: 0.9280\n",
            "Epoch 97/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0539 - accuracy: 0.9804 - val_loss: 0.2586 - val_accuracy: 0.9294\n",
            "Epoch 98/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0578 - accuracy: 0.9787 - val_loss: 0.2466 - val_accuracy: 0.9277\n",
            "Epoch 99/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.2559 - val_accuracy: 0.9267\n",
            "Epoch 100/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0544 - accuracy: 0.9797 - val_loss: 0.2566 - val_accuracy: 0.9281\n",
            "Epoch 101/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0522 - accuracy: 0.9804 - val_loss: 0.2563 - val_accuracy: 0.9280\n",
            "Epoch 102/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0549 - accuracy: 0.9803 - val_loss: 0.2532 - val_accuracy: 0.9278\n",
            "Epoch 103/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0530 - accuracy: 0.9804 - val_loss: 0.2648 - val_accuracy: 0.9283\n",
            "Epoch 104/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0513 - accuracy: 0.9814 - val_loss: 0.2629 - val_accuracy: 0.9292\n",
            "Epoch 105/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0533 - accuracy: 0.9805 - val_loss: 0.2609 - val_accuracy: 0.9262\n",
            "Epoch 106/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0564 - accuracy: 0.9801 - val_loss: 0.2599 - val_accuracy: 0.9264\n",
            "Epoch 107/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0521 - accuracy: 0.9817 - val_loss: 0.2555 - val_accuracy: 0.9281\n",
            "Epoch 108/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0541 - accuracy: 0.9803 - val_loss: 0.2555 - val_accuracy: 0.9290\n",
            "Epoch 109/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0514 - accuracy: 0.9812 - val_loss: 0.2584 - val_accuracy: 0.9296\n",
            "Epoch 110/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0503 - accuracy: 0.9818 - val_loss: 0.2501 - val_accuracy: 0.9283\n",
            "Epoch 111/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0530 - accuracy: 0.9807 - val_loss: 0.2642 - val_accuracy: 0.9263\n",
            "Epoch 112/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0492 - accuracy: 0.9821 - val_loss: 0.2562 - val_accuracy: 0.9304\n",
            "Epoch 113/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0552 - accuracy: 0.9805 - val_loss: 0.2582 - val_accuracy: 0.9280\n",
            "Epoch 114/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0515 - accuracy: 0.9815 - val_loss: 0.2568 - val_accuracy: 0.9276\n",
            "Epoch 115/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0510 - accuracy: 0.9824 - val_loss: 0.2672 - val_accuracy: 0.9287\n",
            "Epoch 116/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0531 - accuracy: 0.9815 - val_loss: 0.2714 - val_accuracy: 0.9271\n",
            "Epoch 117/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0527 - accuracy: 0.9812 - val_loss: 0.2550 - val_accuracy: 0.9278\n",
            "Epoch 118/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0500 - accuracy: 0.9816 - val_loss: 0.2568 - val_accuracy: 0.9292\n",
            "Epoch 119/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0492 - accuracy: 0.9822 - val_loss: 0.2602 - val_accuracy: 0.9299\n",
            "Epoch 120/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.0513 - accuracy: 0.9816 - val_loss: 0.2657 - val_accuracy: 0.9308\n",
            "Epoch 121/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0521 - accuracy: 0.9822 - val_loss: 0.2617 - val_accuracy: 0.9287\n",
            "Epoch 122/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0473 - accuracy: 0.9829 - val_loss: 0.2725 - val_accuracy: 0.9294\n",
            "Epoch 123/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0507 - accuracy: 0.9817 - val_loss: 0.2748 - val_accuracy: 0.9276\n",
            "Epoch 124/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.0474 - accuracy: 0.9827 - val_loss: 0.2813 - val_accuracy: 0.9292\n",
            "Epoch 125/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0505 - accuracy: 0.9817 - val_loss: 0.2829 - val_accuracy: 0.9228\n",
            "Epoch 126/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0459 - accuracy: 0.9840 - val_loss: 0.2798 - val_accuracy: 0.9286\n",
            "Epoch 127/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0459 - accuracy: 0.9836 - val_loss: 0.2880 - val_accuracy: 0.9251\n",
            "Epoch 128/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0487 - accuracy: 0.9830 - val_loss: 0.2701 - val_accuracy: 0.9299\n",
            "Epoch 129/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0454 - accuracy: 0.9842 - val_loss: 0.2761 - val_accuracy: 0.9259\n",
            "Epoch 130/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0497 - accuracy: 0.9824 - val_loss: 0.2722 - val_accuracy: 0.9284\n",
            "Epoch 131/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0499 - accuracy: 0.9830 - val_loss: 0.2766 - val_accuracy: 0.9268\n",
            "Epoch 132/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0488 - accuracy: 0.9830 - val_loss: 0.2703 - val_accuracy: 0.9274\n",
            "Epoch 133/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0455 - accuracy: 0.9842 - val_loss: 0.2777 - val_accuracy: 0.9256\n",
            "Epoch 134/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0435 - accuracy: 0.9852 - val_loss: 0.2618 - val_accuracy: 0.9305\n",
            "Epoch 135/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0500 - accuracy: 0.9814 - val_loss: 0.2742 - val_accuracy: 0.9298\n",
            "Epoch 136/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0499 - accuracy: 0.9822 - val_loss: 0.2798 - val_accuracy: 0.9296\n",
            "Epoch 137/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0449 - accuracy: 0.9845 - val_loss: 0.2643 - val_accuracy: 0.9287\n",
            "Epoch 138/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0442 - accuracy: 0.9844 - val_loss: 0.2944 - val_accuracy: 0.9282\n",
            "Epoch 139/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0478 - accuracy: 0.9831 - val_loss: 0.2814 - val_accuracy: 0.9286\n",
            "Epoch 140/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0506 - accuracy: 0.9820 - val_loss: 0.2744 - val_accuracy: 0.9298\n",
            "Epoch 141/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0472 - accuracy: 0.9843 - val_loss: 0.2776 - val_accuracy: 0.9275\n",
            "Epoch 142/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0442 - accuracy: 0.9846 - val_loss: 0.2764 - val_accuracy: 0.9290\n",
            "Epoch 143/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0436 - accuracy: 0.9846 - val_loss: 0.2714 - val_accuracy: 0.9292\n",
            "Epoch 144/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0466 - accuracy: 0.9840 - val_loss: 0.2837 - val_accuracy: 0.9291\n",
            "Epoch 145/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0509 - accuracy: 0.9824 - val_loss: 0.2913 - val_accuracy: 0.9275\n",
            "Epoch 146/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0455 - accuracy: 0.9836 - val_loss: 0.2793 - val_accuracy: 0.9291\n",
            "Epoch 147/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0425 - accuracy: 0.9850 - val_loss: 0.2944 - val_accuracy: 0.9260\n",
            "Epoch 148/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0457 - accuracy: 0.9840 - val_loss: 0.2866 - val_accuracy: 0.9270\n",
            "Epoch 149/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.0441 - accuracy: 0.9849 - val_loss: 0.2800 - val_accuracy: 0.9303\n",
            "Epoch 150/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.0476 - accuracy: 0.9842 - val_loss: 0.2647 - val_accuracy: 0.9296\n",
            "619/619 [==============================] - 2s 4ms/step - loss: 0.2647 - accuracy: 0.9296\n",
            "\n",
            "SUBMIT THIS BLOCK for the Competition\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.90      0.88      0.89      2051\n",
            "           1       0.99      0.99      0.99      1956\n",
            "           2       0.88      0.91      0.89      2013\n",
            "           3       0.93      0.93      0.93      2008\n",
            "           4       0.90      0.89      0.89      2031\n",
            "           5       1.00      0.98      0.99      1923\n",
            "           6       0.81      0.80      0.80      1952\n",
            "           7       0.95      0.99      0.97      1920\n",
            "           8       0.99      0.98      0.98      1911\n",
            "           9       0.98      0.96      0.97      2035\n",
            "\n",
            "    accuracy                           0.93     19800\n",
            "   macro avg       0.93      0.93      0.93     19800\n",
            "weighted avg       0.93      0.93      0.93     19800\n",
            "\n",
            "Testing Loss: 0.2647133469581604\n",
            "Testing Accuracy: 0.9296464920043945\n",
            "END SUBMISSION BLOCK\n",
            "\n",
            "Model: \"sequential_18\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " conv2d_39 (Conv2D)          (None, 26, 26, 32)        320       \n",
            "                                                                 \n",
            " dropout_57 (Dropout)        (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " activation_51 (Activation)  (None, 26, 26, 32)        0         \n",
            "                                                                 \n",
            " conv2d_40 (Conv2D)          (None, 24, 24, 64)        18496     \n",
            "                                                                 \n",
            " leaky_re_lu_17 (LeakyReLU)  (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " dropout_58 (Dropout)        (None, 24, 24, 64)        0         \n",
            "                                                                 \n",
            " max_pooling2d_19 (MaxPoolin  (None, 12, 12, 64)       0         \n",
            " g2D)                                                            \n",
            "                                                                 \n",
            " activation_52 (Activation)  (None, 12, 12, 64)        0         \n",
            "                                                                 \n",
            " flatten_18 (Flatten)        (None, 9216)              0         \n",
            "                                                                 \n",
            " dense_36 (Dense)            (None, 800)               7373600   \n",
            "                                                                 \n",
            " activation_53 (Activation)  (None, 800)               0         \n",
            "                                                                 \n",
            " dropout_59 (Dropout)        (None, 800)               0         \n",
            "                                                                 \n",
            " dense_37 (Dense)            (None, 10)                8010      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,400,426\n",
            "Trainable params: 7,400,426\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wTXLg2MJMFhy"
      },
      "source": [
        "## Plot the accuracy vs. validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2Z97VRkMIRh",
        "outputId": "e76ae2e1-1eae-4940-8a34-1af9d55b1b6b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 513
        }
      },
      "source": [
        "epoch_list = list(range(1, len(hist.history['accuracy']) + 1))\n",
        "plt.plot(epoch_list, hist.history['accuracy'], epoch_list, hist.history['val_accuracy'])\n",
        "plt.legend((\"Training Accuracy - Best\", \"Validation Accuracy - Best\"))\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_list, hist.history['loss'], epoch_list, hist.history['val_loss'])\n",
        "plt.legend((\"Training Loss - Best\", \"Validation Loss - Best\"))\n",
        "plt.show()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extra Test Section"
      ],
      "metadata": {
        "id": "DFFCgjnAl7SK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#\n",
        "# This is what is known as a Tensorflow (Keras) Sequential model\n",
        "# We will talk at some level about each of these layer types in class.\n",
        "#\n",
        "\n",
        "###Test####\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 #kernel_initializer='he_normal',\n",
        "                 input_shape=input_shape))\n",
        "model_1.add(Dropout(0.9))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Conv2D(64, kernel_size=(3,3)))\n",
        "model_1.add(LeakyReLU(alpha=0.09))\n",
        "model_1.add(Dropout(0.9))\n",
        "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_1.add(Dropout(0.9))\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(800))  \n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Dropout(0.9))\n",
        "model_1.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "my_callbacks_1 = [ModelCheckpoint('model_out.hdf5', monitor='val_accuracy', save_best_only=True, period=1)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GClnAAG4mvpF",
        "outputId": "72fc999f-8d3a-4a71-b5bc-324e39340727"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Flag to determine whether we use Keras' Image augmentation data generator\n",
        "augmentation = False\n",
        "\n",
        "#\n",
        "# Compile the model so we can fit it. Researching loss functions and optimizers\n",
        "# might be a good thing to do.\n",
        "#\n",
        "model_1.compile(loss=keras.losses.categorical_crossentropy, \n",
        "              optimizer=keras.optimizers.Adam(), \n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if not augmentation:\n",
        "    #\n",
        "    # Fit the model.  Once the model is trained we'll evaluate the performance.\n",
        "    print('not using image augmentation')\n",
        "    hist1 = model_1.fit(x_train, y_train,\n",
        "          batch_size=batch_size,\n",
        "          epochs=epochs,\n",
        "          verbose=1,\n",
        "          validation_data=(x_test, y_test),\n",
        "          callbacks=my_callbacks_1)\n",
        "else:\n",
        "# fits the model on batches with real-time data augmentation:\n",
        "    hist1 = model_1.fit_generator(datagen.flow(x_train, y_train, batch_size=batch_size),\n",
        "                           steps_per_epoch=len(x_train) / batch_size, validation_data=(x_test, y_test),\n",
        "                           epochs=epochs, verbose=1, callbacks=my_callbacks_1, workers = 2)\n",
        "\n",
        "\n",
        "score_1 = model_1.evaluate(x_test, y_test)\n",
        "\n",
        "#\n",
        "# Predict on the test data and pass to metrics function\n",
        "yhat = np.argmax(model_1.predict(x_test), axis=-1)\n",
        "y_dec = decode_one_hot(y_test)\n",
        "\n",
        "print(\"\\nSUBMIT THIS BLOCK for the Competition\\n\")\n",
        "print(metrics.classification_report(y_dec, yhat))\n",
        "print(\"Testing Loss:\", score_1[0])\n",
        "print(\"Testing Accuracy:\", score_1[1])\n",
        "print(\"END SUBMISSION BLOCK\\n\")\n",
        "\n",
        "model_1.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ysjXCUMTmAro",
        "outputId": "1ecf2f2e-b2f0-4436-f58f-0b2540ac3c43"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "not using image augmentation\n",
            "Epoch 1/150\n",
            "201/201 [==============================] - 5s 22ms/step - loss: 2.8461 - accuracy: 0.1793 - val_loss: 2.2176 - val_accuracy: 0.1533\n",
            "Epoch 2/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 1.5732 - accuracy: 0.4392 - val_loss: 1.6883 - val_accuracy: 0.5531\n",
            "Epoch 3/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 1.2230 - accuracy: 0.5631 - val_loss: 1.4393 - val_accuracy: 0.5961\n",
            "Epoch 4/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 1.1019 - accuracy: 0.6004 - val_loss: 1.3379 - val_accuracy: 0.6071\n",
            "Epoch 5/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 1.0391 - accuracy: 0.6257 - val_loss: 1.2357 - val_accuracy: 0.6544\n",
            "Epoch 6/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.9831 - accuracy: 0.6413 - val_loss: 1.1850 - val_accuracy: 0.6394\n",
            "Epoch 7/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.9534 - accuracy: 0.6527 - val_loss: 1.1130 - val_accuracy: 0.6352\n",
            "Epoch 8/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.9314 - accuracy: 0.6604 - val_loss: 1.1235 - val_accuracy: 0.6305\n",
            "Epoch 9/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.9138 - accuracy: 0.6663 - val_loss: 1.0622 - val_accuracy: 0.6414\n",
            "Epoch 10/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.8937 - accuracy: 0.6717 - val_loss: 1.1130 - val_accuracy: 0.6313\n",
            "Epoch 11/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8822 - accuracy: 0.6745 - val_loss: 1.0989 - val_accuracy: 0.6210\n",
            "Epoch 12/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.8703 - accuracy: 0.6802 - val_loss: 1.0304 - val_accuracy: 0.6488\n",
            "Epoch 13/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.8672 - accuracy: 0.6790 - val_loss: 0.9732 - val_accuracy: 0.6593\n",
            "Epoch 14/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.8515 - accuracy: 0.6858 - val_loss: 1.0352 - val_accuracy: 0.6365\n",
            "Epoch 15/150\n",
            "201/201 [==============================] - 4s 20ms/step - loss: 0.8485 - accuracy: 0.6852 - val_loss: 0.9614 - val_accuracy: 0.6522\n",
            "Epoch 16/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.8362 - accuracy: 0.6932 - val_loss: 0.9349 - val_accuracy: 0.6667\n",
            "Epoch 17/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8332 - accuracy: 0.6884 - val_loss: 1.0208 - val_accuracy: 0.6201\n",
            "Epoch 18/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8263 - accuracy: 0.6926 - val_loss: 0.9769 - val_accuracy: 0.6463\n",
            "Epoch 19/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8312 - accuracy: 0.6935 - val_loss: 0.9541 - val_accuracy: 0.6494\n",
            "Epoch 20/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8230 - accuracy: 0.6940 - val_loss: 0.9394 - val_accuracy: 0.6592\n",
            "Epoch 21/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8215 - accuracy: 0.6915 - val_loss: 0.8557 - val_accuracy: 0.6894\n",
            "Epoch 22/150\n",
            "201/201 [==============================] - 4s 22ms/step - loss: 0.8256 - accuracy: 0.6959 - val_loss: 0.8974 - val_accuracy: 0.6623\n",
            "Epoch 23/150\n",
            "201/201 [==============================] - 6s 27ms/step - loss: 0.8275 - accuracy: 0.6925 - val_loss: 0.9499 - val_accuracy: 0.6494\n",
            "Epoch 24/150\n",
            "201/201 [==============================] - 4s 21ms/step - loss: 0.8136 - accuracy: 0.6947 - val_loss: 0.8811 - val_accuracy: 0.6858\n",
            "Epoch 25/150\n",
            "201/201 [==============================] - 5s 26ms/step - loss: 0.8146 - accuracy: 0.6962 - val_loss: 0.8721 - val_accuracy: 0.6710\n",
            "Epoch 26/150\n",
            "201/201 [==============================] - 5s 23ms/step - loss: 0.8155 - accuracy: 0.6985 - val_loss: 0.8487 - val_accuracy: 0.6944\n",
            "Epoch 27/150\n",
            "127/201 [=================>............] - ETA: 1s - loss: 0.8162 - accuracy: 0.6970"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-86-67d62ee041b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m           \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m           callbacks=my_callbacks_1)\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# fits the model on batches with real-time data augmentation:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1387\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[0;31m# No error, now safe to assign to logs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1388\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1389\u001b[0;31m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \"\"\"\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 438\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    439\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    440\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    295\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'end'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m       raise ValueError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    316\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 318\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m       \u001b[0mhook\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m       \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1032\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1034\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1035\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1036\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1105\u001b[0m       \u001b[0;31m# Only block async when verbose = 1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1106\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1107\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    561\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 563\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    564\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    912\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    555\u001b[0m     \u001b[0;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 557\u001b[0;31m       \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m     \u001b[0;31m# Strings, ragged and sparse tensors don't have .item(). Return them as-is.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1221\u001b[0m     \"\"\"\n\u001b[1;32m   1222\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1223\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1224\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1187\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1188\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1189\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1190\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1191\u001b[0m       \u001b[0;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epoch_list_1 = list(range(1, len(hist1.history['accuracy']) + 1))\n",
        "plt.plot(epoch_list_1, hist1.history['accuracy'], epoch_list_1, hist1.history['val_accuracy'])\n",
        "plt.legend((\"Training Accuracy - Test\", \"Validation Accuracy - Test\"))\n",
        "plt.show()\n",
        "\n",
        "plt.plot(epoch_list_1, hist1.history['loss'], epoch_list_1, hist1.history['val_loss'])\n",
        "plt.legend((\"Training Loss - Test\", \"Validation Loss - Test\"))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "1RMJEJXEmQ5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compare\n",
        "\n",
        "plt.plot(epoch_list, hist.history['accuracy'], epoch_list, hist.history['val_accuracy'])\n",
        "plt.plot(epoch_list_1, hist1.history['accuracy'], epoch_list_1, hist1.history['val_accuracy'])\n",
        "plt.legend((\"Training Accuracy - Best\", \"Validation Accuracy - Best\", \"Training Accuracy - Test\", \"Validation Accuracy - Test\"))\n",
        "plt.show()\n",
        "\n",
        "#plt.plot(epoch_list_1, hist1.history['loss'], epoch_list_1, hist1.history['val_loss'])\n",
        "#plt.legend((\"Training Loss - Test\", \"Validation Loss - Test\"))\n",
        "#plt.show()\n",
        "\n",
        "\n",
        "print(\"Testing Accuracy - Best:\", score[1])\n",
        "print(\"Testing Accuracy - Test:\", score_1[1])"
      ],
      "metadata": {
        "id": "Btx6l1w8nnN7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C-imOV3gbDji"
      },
      "source": [
        "## Visualization of Performance on the Test Set\n",
        "\n",
        "Here is a visualization of how well our classifier can do inference."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hfsF8TlLwarT"
      },
      "source": [
        "import cv2\n",
        "from imutils import build_montages\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "# initialize our list of output images\n",
        "images = []\n",
        "\n",
        "labelNames = [\"top\", \"trouser\", \"pullover\", \"dress\", \"coat\",\n",
        "\t\"sandal\", \"shirt\", \"sneaker\", \"bag\", \"ankle boot\"]\n",
        " \n",
        "# randomly select a few testing fashion items\n",
        "for i in np.random.choice(np.arange(0, len(y_test)), size=(16,)):\n",
        "\t# classify the clothing\n",
        "\tprobs = model.predict(x_test[np.newaxis, i])\n",
        "\tprediction = probs.argmax(axis=1)\n",
        "\tlabel = labelNames[prediction[0]]\n",
        " \n",
        "\t# extract the image from the testData if using \"channels_first\"\n",
        "\t# ordering\n",
        "\tif K.image_data_format() == \"channels_first\":\n",
        "\t\timage = (x_test[i][0] * 255).astype(\"uint8\")\n",
        " \n",
        "\t# otherwise we are using \"channels_last\" ordering\n",
        "\telse:\n",
        "\t\timage = (x_test[i] * 255).astype(\"uint8\")\n",
        "    # initialize the text label color as green (correct)\n",
        "\tcolor = (0, 255, 0)\n",
        " \n",
        "\t# otherwise, the class label prediction is incorrect\n",
        "\tif prediction[0] != np.argmax(y_test[i]):\n",
        "\t\tcolor = (0, 0, 255)\n",
        " \n",
        "\t# merge the channels into one image and resize the image from\n",
        "\t# 28x28 to 96x96 so we can better see it and then draw the\n",
        "\t# predicted label on the image\n",
        "\timage = cv2.merge([image] * 3)\n",
        "\timage = cv2.resize(image, (96, 96), interpolation=cv2.INTER_LINEAR)\n",
        "\tcv2.putText(image, label, (5, 20), cv2.FONT_HERSHEY_SIMPLEX, 0.75,\n",
        "\t\tcolor, 2)\n",
        " \n",
        "\t# add the image to our list of output images\n",
        "\timages.append(image)\n",
        "# construct the montage for the images\n",
        "montage = build_montages(images, (96, 96), (4, 4))[0]\n",
        " \n",
        "# show the output montage\n",
        "cv2_imshow( montage)\n",
        "cv2.waitKey(0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dMDjuN_anxKM"
      },
      "source": [
        "\n",
        "#Need to check 50/200\n",
        "#epochs = 50                 # Number of Training Epochs\n",
        "#num_classes = 10            # This is the number of classes in the Fashion MNIST dataset\n",
        "#batch_size = 200          # This parameter can be adjusted\n",
        "#img_rows, img_cols = 28, 28 # Pixel sizes of the Images in the Dataset\n",
        "\n",
        "#Current Best\n",
        "epochs = 40                 # Number of Training Epochs\n",
        "num_classes = 10            # This is the number of classes in the Fashion MNIST dataset\n",
        "batch_size = 300          # This parameter can be adjusted\n",
        "img_rows, img_cols = 28, 28 # Pixel sizes of the Images in the Dataset\n",
        "\n",
        "#\n",
        "# This is what is known as a Tensorflow (Keras) Sequential model\n",
        "# We will talk at some level about each of these layer types in class.\n",
        "#\n",
        "\n",
        "###Test####\n",
        "\n",
        "model_1 = Sequential()\n",
        "model_1.add(Conv2D(32, kernel_size=(3, 3),\n",
        "                 activation='relu',\n",
        "                 #kernel_initializer='he_normal',\n",
        "                 input_shape=input_shape))\n",
        "model_1.add(Dropout(0.7))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Conv2D(64, kernel_size=(3,3)))\n",
        "model_1.add(LeakyReLU(alpha=0.05))\n",
        "model_1.add(Dropout(0.6))\n",
        "model_1.add(MaxPooling2D(pool_size=(2,2)))\n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Flatten())\n",
        "model_1.add(Dense(1000))  \n",
        "model_1.add(Activation('relu'))\n",
        "model_1.add(Dropout(0.5))\n",
        "model_1.add(Dense(num_classes, activation=\"softmax\"))\n",
        "\n",
        "my_callbacks = [ModelCheckpoint('model_out.hdf5', monitor='acc', save_best_only=True, period=1)]\n",
        "\n",
        "\n",
        "#Testing Accuracy - Test: 0.9271717071533203"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}